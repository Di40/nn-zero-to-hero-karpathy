# Neural Networks: Zero to Hero

This repository contains my code, notes, and exercises while following Karpathy's [Neural Networks: Zero to Hero](https://karpathy.ai/zero-to-hero.html) lectures. This course covers neural networks and how to build them from scratch in code. The course starts with the basics of backpropagation, then proceeds to multi-layer perceptrons (MLPs), convolutional neural networks (CNNs), and finally builds up to modern deep neural networks like generative pre-trained transformers (GPTs). The course also introduces and covers diagnostic tools for understanding neural networks dynamics and performance. Finally, this course covers the basics of tokenization, byte-pair encoding (BPE), and how to build a tokenizer from scratch. The field of focus in this course is natural language modeling (LM) because not only are language models a superb place to learn deep learning but also most of the skills learned here are immediately transferable to other fields of deep learning such as computer vision (CV). 

Notebooks:
- [Data scraping](notebooks/0_data_scraping_mkd_songs.ipynb)
- [Micrograd](notebooks/1_micrograd.ipynb)
- [Bigram model](notebooks/2_makemore_bigram.ipynb)
- [MLPs and BatchNorm](notebooks/3_makemore_MLP.ipynb)
- [Backpropagation](notebooks/4_makemore_backpropagation.ipynb)
- [Wavenet](notebooks/5_makemore_WaveNet.ipynb)
- [GPT from scratch](notebooks/6_gpt_from_scratch.ipynb)
- [BPE Tokenization](notebooks/7_gpt_tokenizer.ipynb)
- [GPT 2](notebooks/8_gpt2.ipynb)

-----

## References

[micrograd](https://github.com/karpathy/micrograd), [makemore](https://github.com/karpathy/makemore), [GPT](https://github.com/karpathy/ng-video-lecture), [minBPE](https://github.com/karpathy/minBPE)

## License

This repository is licensed under the MIT License. Feel free to use the provided materials for educational purposes, but please attribute the original content to Andrej Karpathy.
