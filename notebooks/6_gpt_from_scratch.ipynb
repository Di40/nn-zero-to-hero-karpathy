{"cells":[{"cell_type":"code","source":["import json\n","import re\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","torch.manual_seed(123)"],"metadata":{"id":"BcZV_dMCmPua","executionInfo":{"status":"ok","timestamp":1736868127770,"user_tz":-60,"elapsed":299,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"72884e9a-2762-48c9-89a4-6931611a048e"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f4516de6770>"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["# Dataset"],"metadata":{"id":"0aIVjaCrmJQx"}},{"cell_type":"markdown","metadata":{"id":"BvXZRv-Gr3MI"},"source":["## Load\n","\n","MKD songs custom dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":306,"status":"ok","timestamp":1736868131821,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"E8ZgZAEFr_Jz"},"outputs":[],"source":["def read_json_songs_to_str(filename='mkd_songs.json'):\n","  with open(filename, 'r', encoding='utf-8') as json_file:\n","    return json.load(json_file)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"HbOag1tbsjDr","outputId":"f73c0530-00cd-42be-ecb3-2521e92a41c7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["298"]},"metadata":{},"execution_count":15}],"source":["songs = []\n","for filename in ['mkd_songs_1.json', 'mkd_songs_2.json', 'mkd_songs_3.json']:\n","  songs.extend(read_json_songs_to_str(filename))\n","len(songs)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"1lAPGa_6tBJr","outputId":"a2bb6ed7-45cd-402b-867e-e5402fbda323"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Денови\\nКако на вратот ѓердани\\nниски камења студени,\\nтака на плешки денови\\nлегнале та натежнале\\nДенови ли се — денови\\nаргатски маки големи!\\nСтани си утре порано\\nдојди си вечер подоцна,\\nнаутро радост понеси\\nнавечер тага донеси —\\nај пуст да е, пуст да би\\nостанал живот кучешки!\\nРоди се човек — роб биди\\nроди се човек — скот умри\\nскотски цел живот работи\\nза други, туѓи имоти.\\nЗа туѓи бели дворови,\\nкопај си црни гробови!\\nЗа себе само ’ргај си\\nза себе маки тргај си —\\nнижи си ѓердан денови\\nнижи си алки ковани,\\nнижи си синџир железен\\nоколу вратот навезен!'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["songs[0]"]},{"cell_type":"markdown","metadata":{"id":"XolvW4u4tn2_"},"source":["The import worked, let's proceed."]},{"cell_type":"markdown","metadata":{"id":"fbdkgvtP1j7R"},"source":["## Data preprocessing"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"fBtxOgFCyi0w","outputId":"2746b1be-60b4-428f-ac86-032d8e3486e0","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["117\n"]}],"source":["#@title Initial vocabulary\n","chars = sorted(list(set(''.join(songs))))\n","vocab_size = len(chars)\n","print(vocab_size)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"9UPGOTB2y3Ii","outputId":"f3bf188a-f9e4-4147-f857-32282abbc747"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !\"'()*,-.0123456789:;?ACHIMOPTV[]`acejopuxyèЃЈЉЌЏАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШабвгдежзиклмнопрстуфхцчшѐёѓѕјљњќѝџ–—‘’“”„…\n"]}],"source":["print(''.join(chars))"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"8G7hHXEd3rz2","outputId":"123d4338-65b1-473f-e9b4-3cd6a14ee876"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":19}],"source":["'è' == 'ѐ'"]},{"cell_type":"markdown","metadata":{"id":"3AZJxNYp2AIo"},"source":["- Replace latinic letters (ACHMOPTV) with corresponding cyrillic letters.\n","- Remove arabic numbers such as 6, 7, 8, etc. from 'Огинот'\n","- Replace '…' (elipsis) with three dots.\n","- Combine '–' (en dash) and '—' (em dash) to just '-' (en dash).\n","- Replace 'ѐ' with 'e' and 'ѝ' with 'и'.\n","  - For a fancier model this shall not be done, but here we can simplify.\n","- Drop: ‘’“”„\n","  - They do not add much value for the given task.\n","- Replace '*' with space.\n","  - There are some lines such as '* * *', which mean nothing except a new line.\n","- Replace 'u' with 'и'.\n","- Remove 'I' and 'V', as they are used for roman numbers, which we don't need.\n","- Remove '2х' and 'x2'.\n","- Replace '6' with 'б'.\n","  - Mistake in the text.\n","\n","And some others..."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"GrGVc9Mj0l47","executionInfo":{"status":"ok","timestamp":1736868132141,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}}},"outputs":[],"source":["# for idx, song in enumerate(songs):\n","#   for char in song:\n","#     if char == 'V':\n","#       print(idx)\n","#       print(song)\n","#       break"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1736868132141,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"boDPNlVB5ctF","cellView":"form"},"outputs":[],"source":["#@title Preprocessing steps\n","processed_songs = []\n","\n","for song in songs:\n","  # Rule 1: Replace Latinic letters (ACHMOPTV) with corresponding Cyrillic letters\n","  latin_to_cyrillic = str.maketrans(\"ACHMOPTacejopxy\", \"АСНМОПТасејорху\")\n","  song = song.translate(latin_to_cyrillic)\n","\n","  # Rule 2: Remove Arabic numbers\n","  song = song.replace('х2', ' ')\n","  song = song.replace('2х', ' ')\n","  song = song.replace('х 2', ' ')\n","  song = song.replace('2 пати', ' ')\n","  song = song.replace('2пати', ' ')\n","  song = song.replace('3х', ' ')\n","  song = re.sub(r'^\\d+\\.\\s*', '\\n', song, flags=re.MULTILINE)\n","  song = re.sub(r'\\d+', '\\n', song)\n","\n","  # Special characters\n","  song = song.replace('…', '...')\n","  song = song.replace(';', ',')\n","  song = re.sub(r'[‘’“”„\"\\'`\\[\\]\\(\\)]', '', song)\n","  song = re.sub(r\"[-–—]\", \"-\", song)\n","  song = song.replace('*', ' ')\n","\n","  # Special letters\n","  song = song.replace('ѐ', 'е').replace('è', 'е').replace('ѝ', 'и').replace('ё', 'е')\n","\n","  # Remaining wrong letters\n","  song = song.replace(\"u\", \"и\")\n","  song = song.replace('6', 'б')\n","\n","  # Rule 9: Replace Roman numbers followed by a dot with a space\n","  song = re.sub(r\"\\b[IV]+\\.\\s*\", \"\", song)\n","  song = re.sub(r\"[IV]\", \"\", song)\n","\n","  # song = re.sub(r'-+\\n', '\\n', song)\n","  song = re.sub(r'^-+\\s*', '', song, flags=re.MULTILINE)\n","  song = re.sub(r'-+\\n', '\\n', song)\n","  song = re.sub(r'^\\.+\\n', '', song, flags=re.MULTILINE)\n","\n","  song = song.replace('Р А Н И', 'РАНИ')\n","  song = song.replace('Н А Р О Д Н И О Т П А БОТНИ К . . .', 'НАРОДНИОТ ПАБОТНИК...')\n","  song = song.replace('Б Р А Т О У Б И Е Ц', 'БРАТОУБИЕЦ')\n","  song = song.replace('П А Т Р И О Т', 'ПАТРИОТ')\n","  song = song.replace('Н А М Е А Н А', 'НА МЕАНА')\n","  song = song.replace('А Ј Д У Ш К О Л И Б Е', 'АЈДУШКО ЛИБЕ')\n","  song = song.replace('Ч О Р Б А Д Ж И СПИРО', 'ЧОРБАДЖИ СПИРО')\n","  song = song.replace('Л Е А Р И Т Е', 'ЛЕАРИТЕ')\n","  song = song.replace('П Р О Т А Т А', 'ПРОТАТА')\n","  song = song.replace('А М А Л О Т', 'АМАЛОТ')\n","  song = song.replace('Ж Е Т В А Р И Т Е', 'ЖЕТВАРИТЕ')\n","  song = song.replace('О Г И Н О Т', 'ОГИНОТ')\n","  song = song.replace('\\nпоема\\n', '\\n')\n","\n","  # Clean up extra spaces and newlines\n","  song = re.sub(r'\\n\\s*\\n+', '\\n\\n', song)\n","\n","  # Add processed song to the list\n","  processed_songs.append(song)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1736868132472,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"IL-qbhZY8GFr","outputId":"e2e35f77-24eb-4001-b39e-733f8615279b","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["68\n"]}],"source":["#@title Final vocabulary\n","chars = sorted(list(set(''.join(processed_songs))))\n","vocab_size = len(chars)\n","print(vocab_size)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1736868132472,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"MWusK11_-4JY","outputId":"6889b18c-b3e9-4da7-aefa-19942c1c9ab9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["298"]},"metadata":{},"execution_count":23}],"source":["len(processed_songs)"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"nne0by5p8UTD","executionInfo":{"status":"ok","timestamp":1736868132472,"user_tz":-60,"elapsed":6,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}}},"outputs":[],"source":["# for idx_song, song in enumerate(processed_songs):\n","#   for idx, char in enumerate(song):\n","#     if char == ')':\n","#       print(idx_song)\n","#       print(song[idx-10:idx+10])\n","#       break"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736868132472,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"ytKXR16o8KG8","outputId":"24ad0ea2-1fd3-4b8c-e77a-87ffb5bdea06"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n"," !,-.:?ЃЈЉЌЏАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШабвгдежзиклмнопрстуфхцчшѓѕјљњќџ\n"]}],"source":["print(''.join(chars))"]},{"cell_type":"code","source":["chars_concat = ''.join(chars)\n","\n","capital_letters = sum(1 for c in chars_concat if c.isupper())\n","lowercase_letters = sum(1 for c in chars_concat if c.islower())\n","special_characters = sum(1 for c in chars_concat if not c.isalnum())\n","\n","print(f'Capital letters: {capital_letters}')\n","print(f'Lower case letters: {lowercase_letters}')\n","print(f'Special characters: {special_characters}')\n","print(f'Total: {capital_letters+lowercase_letters+special_characters}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dProebP7c2Z3","executionInfo":{"status":"ok","timestamp":1736868132472,"user_tz":-60,"elapsed":5,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"7b6f0104-6575-488d-b874-86fb1c5efb46"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Capital letters: 29\n","Lower case letters: 31\n","Special characters: 8\n","Total: 68\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1736868132472,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"},"user_tz":-60},"id":"dzrJeGK_85MI"},"outputs":[],"source":["# for ps in processed_songs:\n","#   print(ps)\n","#   print(50*'-')"]},{"cell_type":"code","source":["type(''.join(processed_songs))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-uh4Z3wF1Yz","executionInfo":{"status":"ok","timestamp":1736868132472,"user_tz":-60,"elapsed":5,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"e039ac5c-aab4-40d9-ad82-b47eb42ee36e"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["str"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["def save_songs_to_json(songs, filename='mkd_songs.json'):\n","    with open(filename, 'w', encoding='utf-8') as json_file:\n","        json.dump(songs, json_file, indent=4, ensure_ascii=False)\n","\n","save_songs_to_json(songs=''.join(processed_songs), filename='mkd_songs_processed.json')"],"metadata":{"id":"fpnVzNr1Fsno","executionInfo":{"status":"ok","timestamp":1736868132472,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Data splitting"],"metadata":{"id":"3nUMy-kjmo1q"}},{"cell_type":"code","source":["#@title Encoder/Decoder\n","# Create a mapping from characters to integers\n","stoi = {s: i for i, s in enumerate(chars)}\n","itos = {i: s for i, s in enumerate(chars)}\n","\n","encode = lambda x: [stoi[c] for c in x]\n","decode = lambda x: ''.join(itos[i] for i in x)\n","\n","print(encode('еј Дејан'))\n","print(decode(encode('еј Дејан')))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9uLjjHr2mrs_","executionInfo":{"status":"ok","timestamp":1736868132472,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"8479ff03-9ff5-4874-cb6b-93a2466fc52e"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["[42, 63, 1, 17, 42, 63, 37, 49]\n","еј Дејан\n"]}]},{"cell_type":"code","source":["text = ''.join(processed_songs)\n","data = torch.tensor(encode(text), dtype=torch.long)  # Encode the text and store it to a tensor\n","print(data.shape)\n","print(data[:100])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"obu5WXylnkEu","executionInfo":{"status":"ok","timestamp":1736868132473,"user_tz":-60,"elapsed":5,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"ca20d57e-96e4-457b-b68d-476bcbdd5d3a"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([203554])\n","tensor([17, 42, 49, 50, 39, 45,  0, 22, 37, 46, 50,  1, 49, 37,  1, 39, 52, 37,\n","        54, 50, 54,  1, 61, 42, 52, 41, 37, 49, 45,  0, 49, 45, 53, 46, 45,  1,\n","        46, 37, 48, 42, 65, 37,  1, 53, 54, 55, 41, 42, 49, 45,  3,  0, 54, 37,\n","        46, 37,  1, 49, 37,  1, 51, 47, 42, 60, 46, 45,  1, 41, 42, 49, 50, 39,\n","        45,  0, 47, 42, 40, 49, 37, 47, 42,  1, 54, 37,  1, 49, 37, 54, 42, 43,\n","        49, 37, 47, 42,  0, 17, 42, 49, 50, 39])\n"]}]},{"cell_type":"code","source":["n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","print(train_data.shape, val_data.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyxRD-CWn5Wh","executionInfo":{"status":"ok","timestamp":1736868132473,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"d5e51746-4e6e-4da7-ceba-16191ec2aa4b"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([183198]) torch.Size([20356])\n"]}]},{"cell_type":"markdown","source":["## Data Loader"],"metadata":{"id":"--dOk2SUoV-n"}},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KorgGb5boLac","executionInfo":{"status":"ok","timestamp":1736868132473,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"dec0f796-f99e-4bea-fc3d-059150a7b2fd"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([17, 42, 49, 50, 39, 45,  0, 22, 37])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size + 1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f'When the input is {context}, the target is {target}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4WrmdvH1obu1","executionInfo":{"status":"ok","timestamp":1736868132473,"user_tz":-60,"elapsed":4,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"7270b8ca-bce1-48a5-b6bf-427c2117cac9"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["When the input is tensor([17]), the target is 42\n","When the input is tensor([17, 42]), the target is 49\n","When the input is tensor([17, 42, 49]), the target is 50\n","When the input is tensor([17, 42, 49, 50]), the target is 39\n","When the input is tensor([17, 42, 49, 50, 39]), the target is 45\n","When the input is tensor([17, 42, 49, 50, 39, 45]), the target is 0\n","When the input is tensor([17, 42, 49, 50, 39, 45,  0]), the target is 22\n","When the input is tensor([17, 42, 49, 50, 39, 45,  0, 22]), the target is 37\n"]}]},{"cell_type":"code","source":["torch.manual_seed(1337)\n","batch_size = 4  # how many independent sequences we'll process in parallel\n","block_size = 8  # what is the maximum context length for predictions\n","\n","def get_batch(split):\n","  # generate a small batch of data of inputs x and targets y\n","  data = train_data if split == 'train' else val_data\n","  ix = torch.randint(len(data) - block_size, (batch_size, ))\n","  x = torch.stack([data[i:i+block_size] for i in ix])\n","  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","  return x, y\n","\n","xb, yb = get_batch('train')\n","print('Inputs:')\n","print(xb.shape)\n","print(xb)\n","print('Targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print(50*'-')\n","\n","for b in range(batch_size):  # batch dimension\n","  for t in range(block_size):  # time dimension\n","    context = xb[b, :t+1]\n","    target = yb[b, t]\n","    print(f'When the input is {context.tolist()}, the target is {target}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CrloWbaoqPCT","executionInfo":{"status":"ok","timestamp":1736868132473,"user_tz":-60,"elapsed":3,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"1896c5fc-cc90-4b69-ec87-721a051a47b8"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Inputs:\n","torch.Size([4, 8])\n","tensor([[37,  3,  0, 37,  1, 51, 37,  1],\n","        [50, 48, 37,  1, 41, 37,  1, 53],\n","        [47, 37, 46, 54, 45,  1, 51, 47],\n","        [ 1, 53, 54, 52, 37, 41, 49, 37]])\n","Targets:\n","torch.Size([4, 8])\n","tensor([[ 3,  0, 37,  1, 51, 37,  1, 63],\n","        [48, 37,  1, 41, 37,  1, 53, 45],\n","        [37, 46, 54, 45,  1, 51, 47, 37],\n","        [53, 54, 52, 37, 41, 49, 37,  1]])\n","--------------------------------------------------\n","When the input is [37], the target is 3\n","When the input is [37, 3], the target is 0\n","When the input is [37, 3, 0], the target is 37\n","When the input is [37, 3, 0, 37], the target is 1\n","When the input is [37, 3, 0, 37, 1], the target is 51\n","When the input is [37, 3, 0, 37, 1, 51], the target is 37\n","When the input is [37, 3, 0, 37, 1, 51, 37], the target is 1\n","When the input is [37, 3, 0, 37, 1, 51, 37, 1], the target is 63\n","When the input is [50], the target is 48\n","When the input is [50, 48], the target is 37\n","When the input is [50, 48, 37], the target is 1\n","When the input is [50, 48, 37, 1], the target is 41\n","When the input is [50, 48, 37, 1, 41], the target is 37\n","When the input is [50, 48, 37, 1, 41, 37], the target is 1\n","When the input is [50, 48, 37, 1, 41, 37, 1], the target is 53\n","When the input is [50, 48, 37, 1, 41, 37, 1, 53], the target is 45\n","When the input is [47], the target is 37\n","When the input is [47, 37], the target is 46\n","When the input is [47, 37, 46], the target is 54\n","When the input is [47, 37, 46, 54], the target is 45\n","When the input is [47, 37, 46, 54, 45], the target is 1\n","When the input is [47, 37, 46, 54, 45, 1], the target is 51\n","When the input is [47, 37, 46, 54, 45, 1, 51], the target is 47\n","When the input is [47, 37, 46, 54, 45, 1, 51, 47], the target is 37\n","When the input is [1], the target is 53\n","When the input is [1, 53], the target is 54\n","When the input is [1, 53, 54], the target is 52\n","When the input is [1, 53, 54, 52], the target is 37\n","When the input is [1, 53, 54, 52, 37], the target is 41\n","When the input is [1, 53, 54, 52, 37, 41], the target is 49\n","When the input is [1, 53, 54, 52, 37, 41, 49], the target is 37\n","When the input is [1, 53, 54, 52, 37, 41, 49, 37], the target is 1\n"]}]},{"cell_type":"markdown","source":["# Simplest baseline: Bigram Language Model"],"metadata":{"id":"TxHsjH23cB7P"}},{"cell_type":"code","source":["#@title Model definition\n","\n","class BigramLanguageModel(nn.Module):\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    # Each token directly reads off the logits for the next token from a lookup table\n","    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","\n","  def forward(self, idx, targets=None):\n","    # idx and targets are both (B,T) tensor of integers\n","    logits = self.token_embedding_table(idx)  # (B,T,C)\n","    # logits are the \"probs\" for the next character\n","    if targets == None:\n","      loss = None\n","    else:\n","      B, T, C = logits.shape\n","      logits = logits.view(B*T, C)\n","      targets = targets.view(B*T)\n","      loss = F.cross_entropy(logits, targets)\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    # idx is (B,T) array of indices in the current context\n","    # we want to predict the next token(s), given the context\n","    for _ in range(max_new_tokens):\n","      # get the predictions\n","      logits, loss = self(idx)  # calls forward\n","      # focus only on the last time step (retrieve the last element in the time dimension)\n","      logits = logits[:, -1, :]  # becomes (B, C)\n","      # apply softmax to get probabilities\n","      probs = F.softmax(logits, dim=-1)  # (B, C)\n","      # sample from the distribution\n","      idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n","      # append sampled index to the running sequence\n","      idx = torch.cat((idx, idx_next), dim=1)  # (B, (T+1))\n","    return idx\n","    # This function at the moment is kinda silly, because we pass a lot of\n","    # history, but use just the last character.\n","    # We do this because we will reuse it for the following models.\n","\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb, yb)\n","print(logits.shape)\n","print(loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vB9BJyrVcDwV","executionInfo":{"status":"ok","timestamp":1736794454445,"user_tz":-60,"elapsed":16,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"ba3428db-c3d6-4889-d490-d7f4291ae733","cellView":"form"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 68])\n","tensor(4.4151, grad_fn=<NllLossBackward0>)\n"]}]},{"cell_type":"markdown","source":["Let's calculate what is the \"random chance\" loss:"],"metadata":{"id":"4loafu-wgHZE"}},{"cell_type":"code","source":["-torch.tensor(1/68).log()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHum9dG2f2Jq","executionInfo":{"status":"ok","timestamp":1736794454445,"user_tz":-60,"elapsed":14,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"d642eaa0-e991-4cb8-b9bf-9920027fbb37"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(4.2195)"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Our loss is a bit higher than this. The initial predictions aren't good, but we will improve ofc. For visualization purposes, let's try to perform inference:"],"metadata":{"id":"llx1L8AmgMlU"}},{"cell_type":"code","source":["print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Um9sFNzrepQN","executionInfo":{"status":"ok","timestamp":1736794454446,"user_tz":-60,"elapsed":12,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"deee328d-e77f-4dcd-dc9e-ac485d0d0da4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","нцЃРљгсљ,нШ.ГХаКтИњшЕАачѕЛШжЃќжк\n","хџкирЖЕфУрИфЌУњлЏБФНаџјЏЖНРгСљ!ЗтазчснгуѕНЈцхПлПГофГЧ:ФџепЗуГкурВжТ\n"]}]},{"cell_type":"markdown","source":["We can confirm that the predictions are \"garbage\"."],"metadata":{"id":"HSOLAPEP3lnj"}},{"cell_type":"code","source":["#@title PyTorch Optimizer\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"TvHnKwB_4JR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["101 % 100"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BptXCNP047wV","executionInfo":{"status":"ok","timestamp":1736794460887,"user_tz":-60,"elapsed":15,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"b31304e5-2533-4b57-a8b7-3b918ea9672b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["batch_size = 32\n","\n","for steps in range(10000):\n","  # sample a new batch\n","  xb, yb = get_batch('train')\n","  # evaluate the loss\n","  logits, loss = m(xb, yb)\n","  optimizer.zero_grad(set_to_none=True)\n","  loss.backward()\n","  optimizer.step()\n","  if steps % 1000 == 0:\n","    print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xXSwA8Vp4Ux8","executionInfo":{"status":"ok","timestamp":1736794482940,"user_tz":-60,"elapsed":22061,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"e54b89a4-95b6-4c27-ee55-38dd0de15481"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["4.624702453613281\n","3.6476426124572754\n","3.115750312805176\n","2.738480567932129\n","2.5497429370880127\n","2.594252347946167\n","2.6938440799713135\n","2.4024441242218018\n","2.5566020011901855\n","2.4998960494995117\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swv2Mcxg5Xki","executionInfo":{"status":"ok","timestamp":1736794482941,"user_tz":-60,"elapsed":43,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"3cd8308a-0bc6-414b-a1d6-afc1bbf48110"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","наљуја..\n","дриднама ми-зем,\n","Ехејдено праловане мејдаре минкречура набра Думлам пистажахам е\n","На!\n","Амини нечадемо,\n","жел Јоили\n","Тешт дЏПаГруата ќенах да.\n","ФТато дајкажунито ти\n","Зачкмикрзрк крдеБЕдено сит.\n","пи ќе сеќ смазојна з лида млегн ќезева ел и \n","Зе етиј готизечако гоќери!\n","Курчеги толаш мо\n","\n","Онори о докри дат т зо\n","Тре, До, дале ра ст.а мошада беја не тк е чи да кум мицеранцив..Дедота зе роданадури де ијанажи намеро ису се пом нено Ма. дл рви?\n","Сте ви,\n","Еле се гото вмо в?И набувра Аја н...\n","Смовеги м м,\n","ни \n"]}]},{"cell_type":"markdown","source":["Dramatic improvements, but we are still far from good."],"metadata":{"id":"McHTyTU_5f3j"}},{"cell_type":"markdown","source":["# The mathematical trick in self attention"],"metadata":{"id":"ZneBAf7ueCPm"}},{"cell_type":"code","source":["B, T, C = 4, 8, 2  # batch, time, channels\n","x = torch.randn(B, T, C)\n","x.shape"],"metadata":{"id":"TkNgdhVFd4Zv","executionInfo":{"status":"ok","timestamp":1736794482941,"user_tz":-60,"elapsed":37,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d438ac4c-9427-42a0-fc73-1fef17eb2daf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["#@title Version 1:\n","# We want x[b,t] = mean_{i<=t} x[b,i]\n","\n","xbow = torch.zeros((B,T,C))  # bag-of-words\n","for b in range(B):\n","  for t in range(T):\n","    xprev = x[b,:t+1]  # (t, C)  -> take everything up to and including the t-th token\n","    xbow[b, t] = torch.mean(xprev, 0)"],"metadata":{"id":"CItCsmSedqz_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovXymi7V9Xtf","executionInfo":{"status":"ok","timestamp":1736794482941,"user_tz":-60,"elapsed":33,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"d0b39826-4b88-47cb-c0b2-88afb85a1405"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.4005,  1.8703],\n","        [-1.6798,  1.2621],\n","        [-0.0083,  1.2187],\n","        [ 0.7576, -0.3020],\n","        [ 0.1721, -1.6713],\n","        [ 0.1849, -0.3305],\n","        [ 0.0101, -1.0563],\n","        [-0.4005,  1.8449]])"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["xbow[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TbqhIKDg-Q4y","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":31,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"b9a2dcd6-9e04-4096-e69e-267f617e39a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.4005,  1.8703],\n","        [-0.6397,  1.5662],\n","        [-0.4292,  1.4504],\n","        [-0.1325,  1.0123],\n","        [-0.0716,  0.4755],\n","        [-0.0288,  0.3412],\n","        [-0.0233,  0.1416],\n","        [-0.0704,  0.3545]])"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["The first row is the same. The second row is average of first and second, the third is average of 1, 2 and 3, etc...\n"],"metadata":{"id":"OBKH6nb2-Z9o"}},{"cell_type":"markdown","source":["The for loops aren't efficient. Let's see the trick now:"],"metadata":{"id":"mRGpL6Bm-wkD"}},{"cell_type":"code","source":["torch.tril(torch.ones(3, 3))  # lower triangular part"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rivp5EPa-9e6","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":28,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"5a669268-ff94-4b0f-dd1f-ecc68eec4579"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 0., 0.],\n","        [1., 1., 0.],\n","        [1., 1., 1.]])"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["a = torch.tril(torch.ones(3, 3))\n","a = a.divide(torch.sum(a, dim=1, keepdim=True))\n","b = torch.randint(0, 10, (3, 2)).float()\n","c = a @ b\n","print('a=')\n","print(a)\n","print('--')\n","print('b=')\n","print(b)\n","print('--')\n","print('c=')\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gX5_10Wa-h4U","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":23,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"7da3e036-4dad-4794-8dea-e46880c4d247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a=\n","tensor([[1.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000],\n","        [0.3333, 0.3333, 0.3333]])\n","--\n","b=\n","tensor([[2., 0.],\n","        [4., 0.],\n","        [7., 3.]])\n","--\n","c=\n","tensor([[2.0000, 0.0000],\n","        [3.0000, 0.0000],\n","        [4.3333, 1.0000]])\n"]}]},{"cell_type":"code","source":["#@title Version 2\n","\n","weights = torch.tril(torch.ones(T, T))\n","weights = weights / weights.sum(1, keepdim=True)\n","weights.shape  # (T, T)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7RI4jgZSAIxq","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":21,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"af062cec-8ce1-44fd-fa74-61351694e1ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([8, 8])"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["xbow2 = weights @ x  # (we will broadcast and add B, T, T) @ (B, T, C) -> (B, T, C)\n","xbow2.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dp0qBNtWAQty","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":17,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"70ee9038-6941-4dcb-ee08-6aa7580195c7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([4, 8, 2])"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["torch.allclose(xbow, xbow2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRjRPMcuAp3D","executionInfo":{"status":"ok","timestamp":1736794482942,"user_tz":-60,"elapsed":14,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"6e4eae54-d967-4337-df65-3922729dbaa9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["#@title Version 3: Use SoftMax\n","\n","tril = torch.tril(torch.ones(T, T))  # lower triangular ones\n","weights = torch.zeros((T, T))\n","weights = weights.masked_fill(tril == 0, float('-inf'))  # Upper triangular part will become -inf <=> The future cannot communicate with the past\n","print(weights)\n","weights = F.softmax(weights, dim=-1)\n","print(weights)\n","xbow3 = weights @ x\n","print(torch.allclose(xbow, xbow3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dNEJ-LeGDKyK","executionInfo":{"status":"ok","timestamp":1736794483392,"user_tz":-60,"elapsed":462,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"4acea33a-b34e-4e13-b363-d1125dcc3e8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n","        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n","        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n","        [0., 0., 0., 0., 0., 0., -inf, -inf],\n","        [0., 0., 0., 0., 0., 0., 0., -inf],\n","        [0., 0., 0., 0., 0., 0., 0., 0.]])\n","tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n","        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n","        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n","        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n","True\n"]}]},{"cell_type":"code","source":["#@title Version 4: Self-attention\n","B, T, C = 4, 8, 32\n","x = torch.randn(B, T, C)\n","\n","# Let's see a single Head perorm self-attention\n","head_size = 16\n","key = nn.Linear(C, head_size, bias=False)  # \"what do I contain\"\n","query = nn.Linear(C, head_size, bias=False)  # \"what am I looking for\" = search request in a database\n","value = nn.Linear(C, head_size, bias=False)\n","\n","k = key(x)  # (B, T, head_size=16)\n","q = query(x)  # (B, T, head_size=16)\n","weights = q @ k.transpose(-2, -1)  # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n","\n","tril = torch.tril(torch.ones(T, T))  # lower triangular ones\n","# weights = torch.zeros((T, T))\n","weights = weights.masked_fill(tril == 0, float('-inf'))  # Upper triangular part will become -inf <=> The future cannot communicate with the past\n","# We don't need prev. line for encoder\n","weights = F.softmax(weights, dim=-1)\n","\n","v = value(x)\n","out = weights @ v  # (B, T, head_size)\n","print(out.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1GFER1NYD5h_","executionInfo":{"status":"ok","timestamp":1736794483394,"user_tz":-60,"elapsed":42,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"80ae4477-bed8-41be-b239-285d83ef4cea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4, 8, 16])\n"]}]},{"cell_type":"code","source":["weights[0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1WI3xNRkKauY","executionInfo":{"status":"ok","timestamp":1736794483395,"user_tz":-60,"elapsed":38,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"c5b2e397-1986-427c-bc28-054782600023"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.3205, 0.6795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.5622, 0.2009, 0.2369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.2093, 0.1084, 0.3560, 0.3263, 0.0000, 0.0000, 0.0000, 0.0000],\n","        [0.0163, 0.1768, 0.7661, 0.0179, 0.0228, 0.0000, 0.0000, 0.0000],\n","        [0.0161, 0.3655, 0.0394, 0.1056, 0.0365, 0.4368, 0.0000, 0.0000],\n","        [0.0450, 0.2589, 0.0459, 0.1734, 0.0923, 0.0251, 0.3595, 0.0000],\n","        [0.2048, 0.1150, 0.0428, 0.1056, 0.0518, 0.0072, 0.3571, 0.1158]],\n","       grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","source":["Notes:\n","- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n","\n","  - Token: Arrows pointing to it\n","    - 1: 1\n","    - 2: 1, 2\n","    - 3: 1, 2, 3\n","    - 4: 1, 2, 3, 4\n","    - etc.\n","\n","- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n","- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other.\n","- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n","- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n","- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"],"metadata":{"id":"Prh8uJ9ZqbnT"}},{"cell_type":"code","source":["k = torch.randn(B, T, head_size)\n","q = torch.randn(B, T, head_size)\n","weights = q @ k.transpose(-2, -1)\n","print(k.var(), q.var(), weights.var())\n","weights = q @ k.transpose(-2, -1) * head_size**-0.5\n","print(k.var(), q.var(), weights.var())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gL_tFF6zPSyx","executionInfo":{"status":"ok","timestamp":1736794483395,"user_tz":-60,"elapsed":35,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"1afcede2-3c75-49d9-fb87-cc64731890e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(0.9661) tensor(1.0400) tensor(15.9833)\n","tensor(0.9661) tensor(1.0400) tensor(0.9990)\n"]}]},{"cell_type":"markdown","source":["The variance is preserved."],"metadata":{"id":"faL-cdAlnjFV"}},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxt0W5MiRNfG","executionInfo":{"status":"ok","timestamp":1736794483395,"user_tz":-60,"elapsed":31,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"b823877f-1047-44ac-f92c-7100a7256384"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TYKp-p_YROil","executionInfo":{"status":"ok","timestamp":1736794483395,"user_tz":-60,"elapsed":28,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"fce33a18-c13c-4cb4-c588-46797a717df1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"]},"metadata":{},"execution_count":49}]},{"cell_type":"markdown","source":["The softmax starts to sharpen towards the highest number, i.e. it will converge towards one-hot encoding quickly (even at initialization)."],"metadata":{"id":"Bu6vZU-GRQSb"}},{"cell_type":"markdown","source":["## LayerNorm"],"metadata":{"id":"zKZLLqoFA0tM"}},{"cell_type":"code","source":["class BatchNorm1d:\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.momentum = momentum\n","    self.training = True\n","    # Parameters (trained with backprop)\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","    # Buffers (trained with a running 'momentum update')\n","    self.running_mean = torch.zeros(dim)\n","    self.running_var = torch.ones(dim)\n","\n","  def __call__(self, x):\n","    # Calculate the forward pass\n","    if self.training:\n","      xmean = x.mean(0, keepdim=True)  # Batch mean\n","      xvar = x.var(0, keepdim=True)  # Batch variance\n","    else:\n","      xmean = self.running_mean\n","      xvar = self.running_var\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    # Update the buffers (Exponential moving average)\n","    if self.training:\n","      with torch.no_grad():\n","        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n","        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]"],"metadata":{"id":"UCsSCdBiA2wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["module = BatchNorm1d(100)\n","x = torch.randn(32, 100)\n","x = module(x)\n","print(x.shape)\n","print(x[:,0].mean(), x[:,0].std())  # mean, std of one feature across all batch inputs\n","print(x[0,:].mean(), x[0,:].std())  # mean, std of a single input from the batch, of its features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fLWgW79FA4ux","executionInfo":{"status":"ok","timestamp":1736794584980,"user_tz":-60,"elapsed":438,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"4abcbc5b-9ae7-4f39-97e4-9050ff192cf4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 100])\n","tensor(2.9802e-08) tensor(1.0000)\n","tensor(-0.0935) tensor(1.0192)\n"]}]},{"cell_type":"code","source":["class LayerNorm1d:  # (used to be BatchNorm1d)\n","\n","  def __init__(self, dim, eps=1e-5, momentum=0.1):\n","    self.eps = eps\n","    self.gamma = torch.ones(dim)\n","    self.beta = torch.zeros(dim)\n","\n","  def __call__(self, x):  # calculate the forward pass\n","    # just change 0 to 1\n","    xmean = x.mean(1, keepdim=True) # batch mean\n","    xvar = x.var(1, keepdim=True) # batch variance\n","    xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # normalize rows to unit variance\n","    self.out = self.gamma * xhat + self.beta\n","    return self.out\n","\n","  def parameters(self):\n","    return [self.gamma, self.beta]\n","\n","torch.manual_seed(1337)\n","module = LayerNorm1d(100)\n","x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n","x = module(x)\n","print(x.shape)\n","print(x[:,0].mean(), x[:,0].std())\n","print(x[0,:].mean(), x[0,:].std())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jEAsOmDDAbgn","executionInfo":{"status":"ok","timestamp":1736794722222,"user_tz":-60,"elapsed":881,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"f33e0f44-b9b2-4fb8-cf14-65c9c4a69c7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 100])\n","tensor(0.1469) tensor(0.8803)\n","tensor(-9.5367e-09) tensor(1.0000)\n"]}]},{"cell_type":"markdown","source":["# GPT: Whole code"],"metadata":{"id":"PYNyGHmcZD-x"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","import json\n","\n","# Hyperparameters\n","batch_size = 64  # How many independent sequences will we process in parallel?\n","block_size = 256  # What is the maximum context length for predictions?\n","max_iters = 5000\n","eval_interval = 500\n","learning_rate = 3e-4  # bigger NN => smaller learning rate\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","n_embed = 384  # embedding dimension\n","n_head = 6  # 384/6 = 64 => each head will be 64-dimensional\n","n_layer = 6\n","dropout = 0.2\n","\n","# ------------\n","\n","torch.manual_seed(123)\n","\n","def read_json_songs_to_str(filename='mkd_songs.json'):\n","    with open(filename, 'r', encoding='utf-8') as json_file:\n","        return json.load(json_file)\n","text = read_json_songs_to_str('mkd_songs_processed.json')\n","\n","# Vocabulary (all the unique characters that occur in this text)\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print('Vocabulary size:', vocab_size)\n","# Encoder/Decoder: Create a mapping from characters to integers\n","stoi = { ch:i for i,ch in enumerate(chars) }\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda x: [stoi[c] for c in x]  # Encoder: take a string, output a list of integers\n","decode = lambda x: ''.join([itos[i] for i in x])  # Decoder: take a list of integers, output a string\n","\n","# Train and test splits\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data))  # The first 90% will be train, the rest val\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","# Data loader\n","def get_batch(split):\n","    # Generate a small batch of data of inputs x and targets y\n","    data = train_data if split == 'train' else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x, y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            _, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","class Head(nn.Module):\n","    '''one head of self-attention'''\n","    def __init__(self, head_size):\n","        super().__init__()\n","        self.key = nn.Linear(n_embed, head_size, bias=False)\n","        self.query = nn.Linear(n_embed, head_size, bias=False)\n","        self.value = nn.Linear(n_embed, head_size, bias=False)\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # to add a variable tril to the model\n","        self.dropout = nn.Dropout(dropout)  # to prevent some of the nodes from randomly communicating\n","\n","    def forward(self, x):\n","        B, T, C = x.shape\n","        k = self.key(x)  # (B, T, C)\n","        q = self.query(x)  # (B, T, C)\n","        v = self.value(x)   # (B, T, C)\n","        # Compute attention scores (\"affinities\")\n","        wei = q @ k.transpose(-2, -1) * C**(-0.5)  # (B, T, C) @ (B, C, T) -> (B, T, T)\n","        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # decoder block\n","        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n","        wei = self.dropout(wei)\n","        # Perform the weighted aggregation of the values\n","        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n","        return out\n","\n","class MultiHeadAttention(nn.Module):\n","    ''' multiple heads of self-attention in parallel '''\n","    def __init__(self, num_heads, head_size):\n","        super().__init__()\n","        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n","        self.proj = nn.Linear(n_embed, n_embed)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concatenate the outputs of all heads that run in parallel\n","        out = self.proj(out)  # linear projection\n","        out = self.dropout(out)\n","        return out\n","\n","class FeedForward(nn.Module):\n","    ''' a simple linear layer followed by non-linearity'''\n","    def __init__(self, n_embed):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(n_embed, 4 * n_embed),  # 4 times the embedding size, as in the original transformer\n","            nn.ReLU(),\n","            nn.Linear(4 * n_embed, n_embed),  # projection\n","            nn.Dropout(dropout)\n","        )\n","    def forward(self, x):\n","        return self.net(x)\n","\n","class Block(nn.Module):\n","    ''' A transformer block: communication followed by computation '''\n","    def __init__(self, n_embed, n_head):\n","        # n_embed: embedding dimension, n_head: the number of heads we'd like\n","        super().__init__()\n","        head_size = n_embed // n_head\n","        self.sa = MultiHeadAttention(num_heads=n_head, head_size=head_size)\n","        self.ffwd = FeedForward(n_embed)\n","        self.ln1 = nn.LayerNorm(n_embed)\n","        self.ln2 = nn.LayerNorm(n_embed)\n","\n","    def forward(self, x):\n","        # We will apply Layer norm before passing through the corresponding module\n","        # (this is a deviation from the original paper but it is more common now)\n","        x = x + self.sa(self.ln1(x))  # Residual connection\n","        x = x + self.ffwd(self.ln2(x))  # Residual connection\n","        return x\n","\n","class GPTLanguageModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","        # Each token directly reads off the logits for the next token from a lookup table\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)  # (V, C)\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)  # (T, C)\n","        self.blocks = nn.Sequential(*[Block(n_embed, n_head=4) for _ in range(n_layer)])\n","        self.ln_f = nn.LayerNorm(n_embed)  # final layer norm\n","        self.lm_head = nn.Linear(n_embed, vocab_size)  # (C, V)\n","\n","    def forward(self, idx, targets=None):\n","        # idx and targets are both (B, T) tensor of integers\n","        B, T = idx.shape\n","        token_emb = self.token_embedding_table(idx)  # (B, T, C)\n","        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n","        x = token_emb + pos_emb  # (B, T, C) + (T, C) -> (B, T, C)\n","        x = self.blocks(x)\n","        x = self.ln_f(x)\n","        logits = self.lm_head(x)  # (B, T, C)\n","        # logits are the \"probs\" for the next character\n","        if targets == None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C)\n","            targets = targets.view(B*T)\n","            loss = F.cross_entropy(logits, targets)\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is (B,T) array of indices in the current context\n","        # we want to predict the next token(s), given the context\n","        for _ in range(max_new_tokens):\n","            # crop idx to the last block_size tokens\n","            idx_cond = idx[:, -block_size:]\n","            # get the predictions\n","            logits, loss = self(idx_cond)  # calls forward\n","            # focus only on the last time step (retrieve the last element in the time dimension)\n","            logits = logits[:, -1, :]  # becomes (B, C)\n","            # apply softmax to get probabilities\n","            probs = F.softmax(logits, dim=-1)  # (B, C)\n","            # sample from the distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n","            # append sampled index to the running sequence\n","            idx = torch.cat((idx, idx_next), dim=1)  # (B, (T+1))\n","        return idx\n","        # This function at the moment is kinda silly, because\n","        # we pass a lot of  history, but use just the last character.\n","        # We do this because we will reuse it for the following models.\n","\n","model = GPTLanguageModel().to(device)\n","\n","# Create a PyTorch optimizer\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters):\n","\n","    # every once in a while evaluate the loss on train and val sets\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(500*'-')\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        # Generate from the model\n","        context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","        print(decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n","\n","    # Sample a new batch of data\n","    xb, yb = get_batch('train')\n","\n","    # evaluate the loss\n","    logits, loss = model(xb, yb)\n","    optimizer.zero_grad(set_to_none=True)\n","    loss.backward()\n","    optimizer.step()\n","\n","# Generate from the model\n","context = torch.zeros((1, 1), dtype=torch.long, device=device)\n","print(decode(model.generate(context, max_new_tokens=1000)[0].tolist()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LprfJ8jkY49p","executionInfo":{"status":"ok","timestamp":1736877082066,"user_tz":-60,"elapsed":2939504,"user":{"displayName":"Dejan Dichoski","userId":"14643339944700468762"}},"outputId":"9effb0c2-fab1-4158-e726-2c21e6eeca2d"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 68\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 0: train loss 4.4301, val loss 4.4271\n","\n","орљЧожИнЃтлпќПќ?ОпнЦЏЌ!,вј-ЖСЈРУр:жГцосхшаИЃЧљНх!пфПЈАЏЛѓѕЉвзГ.ѓЗ дУубдзињАлжАЛџРЧИиљсмбСИшСаЈ!ЃКџрЧ\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 500: train loss 2.2438, val loss 2.3122\n","\n","Суна е памат наа Анице мерџа ефан, Довеси,\n","Бе пофтефе со омле,\n","Ле јт оф пчет, ме прелино спуша мо,\n","Д\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 1000: train loss 1.6628, val loss 1.8578\n","\n","амченах врах на на пратлихнавт,\n","по не мела платнегне,\n","Вда прртугнави водини!\n","Захо по Стргна пратен у\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 1500: train loss 1.2045, val loss 1.6788\n","\n","покуси грло\n","Депчеха пољусна,\n","Ми гдемало либе\n","Двајца не му да и преп.\n","Ајде дали му\n","Кој ти славај мајк\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 2000: train loss 0.8328, val loss 1.7946\n","\n","тор на црне, горо,\n","земе на е братко, плака,\n","Всега маки.\n","Свеко да братко, почи и три минцови\n","а гради \n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 2500: train loss 0.5163, val loss 2.0596\n","\n","идаф крлети испрвта,\n","О твојата слзи будухте,\n","грмно и пророј, шуми раниот!\n","\n","Шумот је бил, прикале\n","ен \n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 3000: train loss 0.2887, val loss 2.3827\n","\n","-полни стегнал,\n","светат маки души в таурчи,\n","браќа цел на срце жнее\n","от ради в портење.\n","\n","Секоја немаш с\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 3500: train loss 0.1740, val loss 2.7028\n","\n","рамна ке те излажам\n","за верите славја\n","на моево врваве\n","стаха му на Тори чедо.\n","Слушај, синко, пушка сја\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 4000: train loss 0.1267, val loss 2.9846\n","\n","Лика си ја да си ја дојдам.\n","Што не слушај, пофа ќе си ја ме\n","Македонски ја зарпот тамба.\n","Тагине от ер\n","--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","step 4500: train loss 0.1048, val loss 3.1717\n","\n","милома мома мамо ино за до ме нападар,\n","чекај ми е мило моме,\n","бегај ми е мило да коњче,\n","Оф аман, оф а\n","\n","Со си зелени, мајкин стресојте,\n","Отвој ми леле, севдо мори\n","мавме клета.  \n","Ајде купијам извадина\n","вадиони шета ми долж да коленам\n","без леб да вратиш лице?\n","Ајде едестраш ли гори,\n","удрел еден брег на широка.  \n","Не ми реме сакаш бре,\n","сака да си растрмаш\n","улав ти род кара...\n","Оште и вликиот \n","Наљути мајкиот морена,\n","настанах, настах\n","иродни на душата\n","и см жени мја даде\n","салнуто си мајка\n","туку, мајко ќе назвезе\n","женеки по тагата потегне\n","и белки да го тагата неме.\n","\n","Ако умра им има имат ми иди\n","срце што животини,\n","не што се да одават,\n","над тага да драгат\n","и на утринта ружи свет\n","да не радост потегната!\n","\n","Ста ме, од викаш, одавна доба\n","на чедено утро да оди\n","дод грабит денот мои.\n","И ке ке чува цути не ми е?\n","Зар велит: бујнак да бегат,\n","и ли бегат, мили веднаш, зар бегат,\n","радо ви примкна падни силна,\n","душа да буриче да врлика.\n","Сега нек ога светит:\n","\n","Стрите тешки Боже светели Маре Помоноса,\n","Го дружи сестрит ние.\n","Кога виде секо црна, коња јачи\n","Да преку сака се стоит снитите \n","а мојте лудо ни срце\n","и дружи што ми градини\n","как\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"-VDoPH6VgqJR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Up to 1500 steps training is going well, after that the model starts to overfit."],"metadata":{"id":"8-rHnUoegx7-"}},{"cell_type":"markdown","source":["Vocabulary size: 68\n","\n","\n","1. step 0: train loss 4.4301, val loss 4.4271\n","\n","  орљЧожИнЃтлпќПќ?ОпнЦЏЌ!,вј-ЖСЈРУр:жГцосхшаИЃЧљНх!пфПЈАЏЛѓѕЉвзГ.ѓЗ дУубдзињАлжАЛџРЧИиљсмбСИшСаЈ!ЃКџрЧ\n","\n","2. step 500: train loss 2.2438, val loss 2.3122\n","\n","  Суна е памат наа Анице мерџа ефан, Довеси,\\\n","  Бе пофтефе со омле,\\\n","  Ле јт оф пчет, ме прелино спуша мо,\\\n","  Д\n","\n","3. step 1000: train loss 1.6628, val loss 1.8578\n","\n","  амченах врах на на пратлихнавт,\\\n","  по не мела платнегне,\\\n","  Вда прртугнави водини!\\\n","  Захо по Стргна пратен у\n","\n","4. step 1500: train loss 1.2045, val loss 1.6788\n","\n","  покуси грло\\\n","  Депчеха пољусна,\\\n","  Ми гдемало либе\\\n","  Двајца не му да и преп.\\\n","  Ајде дали му\\\n","  Кој ти славај мајк\n","\n","5. step 2000: train loss 0.8328, val loss 1.7946\n","\n","  тор на црне, горо,\\\n","  земе на е братко, плака,\\\n","  Всега маки.\\\n","  Свеко да братко, почи и три минцови\\\n","  а гради\n","\n","6. step 2500: train loss 0.5163, val loss 2.0596\n","\n","  идаф крлети испрвта,\\\n","  О твојата слзи будухте,\\\n","  грмно и пророј, шуми раниот!\\\n","\\\n","  Шумот је бил, прикале\\\n","  ен\n","\n","7. step 3000: train loss 0.2887, val loss 2.3827\n","\n","  -полни стегнал,\\\n","  светат маки души в таурчи,\\\n","  браќа цел на срце жнее\\\n","  от ради в портење.\\\n","\\\n","  Секоја немаш с\\\n","\n","8. step 3500: train loss 0.1740, val loss 2.7028\n","\n","  рамна ке те излажам\\\n","  за верите славја\\\n","  на моево врваве\\\n","  стаха му на Тори чедо.\\\n","  Слушај, синко, пушка сја\n","\n","9. step 4000: train loss 0.1267, val loss 2.9846\n","\n","  Лика си ја да си ја дојдам.\\\n","  Што не слушај, пофа ќе си ја ме\\\n","  Македонски ја зарпот тамба.\\\n","  Тагине от ер\n","\n","10. step 4500: train loss 0.1048, val loss 3.1717\n","\n","  милома мома мамо ино за до ме нападар,\\\n","  чекај ми е мило моме,\\\n","  бегај ми е мило да коњче,\\\n","  Оф аман, оф а\n","\n","\n","Post-training results (5000 tokens):\n","\n","  Со си зелени, мајкин стресојте,\\\n","  Отвој ми леле, севдо мори\\\n","  мавме клета.\\  \n","  Ајде купијам извадина\\\n","  вадиони шета ми долж да коленам\\\n","  без леб да вратиш лице?\\\n","  Ајде едестраш ли гори,\\\n","  удрел еден брег на широка.\\  \n","  Не ми реме сакаш бре,\\\n","  сака да си растрмаш\\\n","  улав ти род кара...\\\n","  Оште и вликиот \\\n","  Наљути мајкиот морена,\\\n","  настанах, настах\\\n","  иродни на душата\\\n","  и см жени мја даде\\\n","  салнуто си мајка\\\n","  туку, мајко ќе назвезе\\\n","  женеки по тагата потегне\\\n","  и белки да го тагата неме.\\\n","\\\n","  Ако умра им има имат ми иди\\\n","  срце што животини,\\\n","  не што се да одават,\\\n","  над тага да драгат\\\n","  и на утринта ружи свет\\\n","  да не радост потегната!\\\n","\\\n","  Ста ме, од викаш, одавна доба\\\n","  на чедено утро да оди\\\n","  дод грабит денот мои.\\\n","  И ке ке чува цути не ми е?\\\n","  Зар велит: бујнак да бегат,\\\n","  и ли бегат, мили веднаш, зар бегат,\\\n","  радо ви примкна падни силна,\\\n","  душа да буриче да врлика.\\\n","  Сега нек ога светит:\\\n","\\\n","  Стрите тешки Боже светели Маре Помоноса,\\\n","  Го дружи сестрит ние.\\\n","  Кога виде секо црна, коња јачи\\\n","  Да преку сака се стоит снитите \\\n","  а мојте лудо ни срце\\\n","  и дружи што ми градини\\\n","  как\\"],"metadata":{"id":"li_Nt3doe-sG"}},{"cell_type":"markdown","source":[],"metadata":{"id":"GAfHzB9Qe1v7"}}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["0aIVjaCrmJQx","BvXZRv-Gr3MI","3nUMy-kjmo1q","--dOk2SUoV-n","TxHsjH23cB7P","ZneBAf7ueCPm"],"gpuType":"T4","authorship_tag":"ABX9TyNlZs4zLbMJXdO/3vfylBqZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}